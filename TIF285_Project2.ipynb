{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2: Newton vs the machine: <br/>*solving the chaotic three-body problem using deep neural networks*\n",
    "## Learning from data [TIF285], Chalmers, Fall 2022\n",
    "\n",
    "Last revised: 10-Oct-2022 by Christian Forss√©n [christian.forssen@chalmers.se] and Shahnawaz Ahmed [shahnawaz.ahmed@chalmers.se]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- See deadline on the course web page\n",
    "- This project is performed in groups of two students. \n",
    "- The second part of the project is optional. See examination rules on the course web page.\n",
    "- Hand-in your written report and your solution source code via Canvas. \n",
    "- Please also upload your ANN model with the trained parameters as `myNN.h5` using the keras `save_model` method: `tf.keras.models.save_model(model, 'myNN.h5')`, which employs the compact, binary [hdf5](https://www.hdfgroup.org/solutions/hdf5/) file format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Written report\n",
    "- Page limit: 6 pages (excluding title page and list of references). 3 extra pages are allowed when doing also the optional extra task.\n",
    "- Give a short description of the nature of the problem and the methods you have used.\n",
    "- Include your results either in figure form or in a table. All tables and figures should have relevant captions and labels on the axes.\n",
    "- Try to give an interpretation of you results.\n",
    "- Upload the source code of your program as a separate file (.ipynb or .py). Comment your program properly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning goals:\n",
    "* Apply deep neural networks to build an emulator for a physics problem that is difficult and costly to solve numerically.\n",
    "* Get familiar with [tensorflow](https://www.tensorflow.org/), the popular open-source library to train ML models created by the Google Brain team.\n",
    "* Get experience with some diagnostics used in the training of neural networks. \n",
    "* Understand the difficulties and choices involved when training a deep neural network.\n",
    "* This a less-guided set of tasks and you will have to put together ideas and tools we've discussed.\n",
    "* Reproduce scientific results from a published paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main task\n",
    "The overall project goal is to reproduce various results in a paper: [*Newton vs the machine: solving the chaotic three-body problem using deep neural networks*](https://arxiv.org/abs/1910.07291) by Philip G. Breen, Christopher N. Foley, Tjarda Boekholt, Simon Portegies Zwart.\n",
    "\n",
    "The authors have graciously agreed that we use the raw data from their study for this project (but please don't distribute any further)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Physical problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project we are studying the motion of three bodies of equal mass that move under the mutual influence of classical gravitational forces. Due to the system's chaotic nature, the solution to this problem for a given initialization can only be found by performing laborious iterative numerical calculations that have unpredictable and potentially infinite computational cost. \n",
    "\n",
    "Here you will demonstrate that a deep artificial neural network (ANN) is able to learn the solutions to the equations of motion from a set of numerical training data. This ANN can then provide good approximations for new initial conditions at a fixed (and small) computational cost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training data that we will be using was generated with a state-of-the-art numerical code `Brutus` [[Boekholt, T., Portegies Zwart, S. On the reliability of N-body simulations. Comput. Astrophys. 2, 2 (2015).](https://doi.org/10.1186/s40668-014-0005-3)]. The run time of that code for the trajectories studied here ranges from minutes up to hours. In the end, it will therefore be relevant to quantify what is the computational cost of making a prediction with our constructed ANN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial condition and choice of coordinates\n",
    "- We work in the the center-of-mass (CM) frame and only consider the scenario in which the three masses start from rest.\n",
    "- It can be shown that, without loss of generality, it is always possible to choose a coordinate system such that the three particles $p_1, p_2, p_3$ move in a plane so that just $(x,y)$ coordinates are needed.\n",
    "- Furthermore, the $(x,y)$ coordinates can be chosen and scaled such that particle 1 starts in position $(1,0)$ and the initial position of particle 2 is located in the second quantile ($x<0$, $y>0$).\n",
    "- Since we are in the CM frame and our systems consists of three equal masses we will always have $x_1+x_2+x_3=0$ and $y_1+y_2+y_3=0$. Consequently, we can determine the position of particle 3 from knowledge of the positions of particles 1 and 2.\n",
    "\n",
    "See also Fig. 1 in the paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "The data that you should use is made available as a compressed numpy file on the Canvas course page. \n",
    "\n",
    "Navigate to Files > Project2  and download `data_project2.npz` (or directly via this link:\n",
    "<a href=\"https://chalmers.instructure.com/files/1674468/download?download_frd=1\" title=\"data_project2.npz\" class=\"instructure_file_link\">data_project2.npz</a>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load modules\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Add your own import statements if needed\n",
    "\n",
    "from copy import deepcopy #Is this module allowed?\n",
    "import math\n",
    "from keras.callbacks import CSVLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and unpack a compressed npy array\n",
    "load_data = np.load('data_project2.npz')\n",
    "data = load_data['arr_0']\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The first axis labels the 9000 data samples.\n",
    "- The second axis is the 1000 time steps, which corresponds to 3.9 time units. The original data is composed of 2561 time steps, reaching 10 time units, but as discussed in the paper the training of the ANN works better for the somewhat shorter time interval.\n",
    "- The nine columns of the last axis correspond to: $[t, x_1, y_1, x_2, y_2, v_{x,1}, v_{y,1}, v_{x,2}, v_{y,2}]$ (note again that the position of particle 3 can be obtained from the positions of particles 1 and 2). The velocity data is only used in the extra task.\n",
    "- Some of the trajectories contain collisions such that the positions of the particles get stuck at (0, 0). Check the data to find those trajectories and remove them before training your model.\n",
    "- Note that the trajectories are represented by data points in sets of 1000 (time steps). While creating batches of data, make sure you are not mixing part of the data from one trajectory with a different trajectory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List of tasks\n",
    "1. Download and load the pre-trained ANN from the paper (see below) and construct your own ANN using the provided data. Compare the complexity (number of trainable parameters) in the pre-trained versus your own ANN.\n",
    "1. Show the Mean Absolute Error (MAE) vs epoch from the training of your own ANN (as in Fig. 3 of the paper). I.e. you should show both the MAE metric on the training and the validation data.\n",
    "1. Check the accuracy of the pre-trained ANN by showing some trajectories and the ANN prediction as in Fig. 4 of the paper. Add also the trajectory of your ANN and comment on its performance.\n",
    "1. Check the computational cost of making a prediction with the ANN. You will find that it is much shorter than the numerical integration performed by `Brutus` (which according to the reference takes minutes to hours for finding a trajectory).\n",
    "1. Having access to these ANN emulators you can explore the chaotic aspect of this motion by creating a hundred different trajectories from a slightly disturbed initial condition. Try to reproduce Fig. 5 in the paper (both with the pre-trained and your own ANN)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Suggestions for how to proceed:\n",
    "* Follow the lead of the demo notebook for using tensorflow / keras (plus see the hints below).\n",
    "* First download and load the data set and the ANN from the paper (both available on the Canvas course page. Navigate to Files > Project2; see hint below)\n",
    "* It is suggested (but not required) to create helper functions (see hints below): \n",
    "  * `get_data` for extracting a trajectory (positions of particle 1 as a function of the discrete time) from the data; \n",
    "  * `get_trajectories` for generating the positions of all particles from a prediction output that contains the arrays of the first two particles' positions; \n",
    "  * `plot_trajectories` for plotting a trajectory.\n",
    "* When creating your own ANN it is suggested that you first design a smaller one than in the paper (e.g., using five dense layers with 64 neurons in each) and analyse how it performs by looking at the training vs validation error. The creation of a large network with ten layers and 128 nodes per layer is optional (and might require more cpu-hours for training).\n",
    "* Try first with the `relu` activation function and the `Adam` optimizer with a learning rate of 0.001 and moments of 0.5 (these hyper-parameters can easily be changed if you want; see hints below). \n",
    "    - The choice of optimizer  and hyper-parameters such as learning rate can certainly influence your results. For the interested, there is a recent extensive guide with open-source experimental results published here: https://arxiv.org/abs/2007.01547.\n",
    "* Use 90% of the data for training and 10% for validation. You probably need at least hundred epochs for training and it is suggested to split the data into 10-100 batches (see hints below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments and hints\n",
    "* Download the pre-trained ANN model (<a href=\"https://chalmers.instructure.com/files/1674480/download?download_frd=1\" title=\"Breen_NN_project2.h5\" class=\"instructure_file_link\">Breen_NN_project2.h5</a>) and load it with: `pre_trained_model = keras.models.load_model(\"Breen_NN_project2.h5\")`. This file is also included in the gzipped tarball that contains the project 2 files.\n",
    "* The ANN input should be an array with three elements: $[t,x_2,y_2]$, where the last two will be the initial position of particle 2 and the first one is the time step for which the particle positions should be predicted. Remember that particle 1 always starts in $(1,0)$. \n",
    "* Input data corresponding to a single trajectory is then of shape $1000 \\times 3$ where the first column contains the time steps while the $x_2,y_2$ columns are repetitions of the initial position.\n",
    "* I.e., the inputs to the `tensorflow` network is a tensor with three columns where the second and third columns have values that are repeated (with values of the initial position at [t=0]) for all the rows corresponding to a single trajectory.\n",
    "* The output should be an array with four elements $[x_1,y_1,x_2,y_2]$ that corresponds to the positions of particles 1 and 2 at the time step $t$ from the input. Again, we can consider a tensor (batch) of outputs corresponding, e.g., to a full trajectory.\n",
    "* As mentioned, some of the trajectories contain collisions such that the positions of the particles get stuck at (0, 0). Check the data to find those trajectories and remove them before training your model.\n",
    "* The data can be turned into a `tensorflow` dataset using: `dataset = tf.data.Dataset.from_tensor_slices((x, y))`, where `x` and `y` are input and output data, respectively (you might possibly need to reshape using `x.reshape(-1, 3)` and `y.reshape(-1, 4)`)\n",
    "* Having created a dataset, e.g. `train_dataset`, you can make batches (e.g. with batchsize 5000) with `train_dataset = train_dataset.batch(5000)`. The same should then be done for the validation data. As mentioned, the trajectories are represented by data points in sets of 1000 (time steps). While creating batches of data, make sure you are not mixing part of the data from one trajectory with a different trajectory. \n",
    "* When compiling a `tensorflow` model, with `model.compile()`, the Adam optimizer can be specified via the argument `optimizer=keras.optimizers.Adam(0.001, 0.5, 0.5)`\n",
    "* Furthermore, the loss function can be chosen using the argument `loss=keras.losses.MeanAbsoluteError()`\n",
    "* Fitting the model for 100 epochs with specified training and validation data sets can be performed with: `model.fit_generator(train_dataset, epochs=100, validation_data = test_dataset)`\n",
    "* Given the cost of training the network with all data, it is not required to make extensive tests of tuning the hyperparameters.\n",
    "* More exact timing of a function can be measured using [\"Jupyter notebook magic\"](https://towardsdatascience.com/the-top-5-magic-commands-for-jupyter-notebooks-2bf0c5ae4bb8) by putting the command to be measured in a notebook cell that starts with `!timeit`. This will run the cell many times and report the mean and standard deviation of the evaluation time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper function docstrings\n",
    "The following are just suggestions. You are allowed to construct a solution using your own helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_filter(data):\n",
    "    '''\n",
    "    Identifies converging trajectories within the provided array of data and returns\n",
    "    and filters it out. Only suited for data of the shape (9000,1000,9).\n",
    "\n",
    "    Args:\n",
    "        Data (array): Array of trajectory data of the shape (9000,1000,9) where\n",
    "        - index 0 = Amount of data samples\n",
    "        - index 1 = Amount of time steps\n",
    "        - index 2 = Motion variables [t,x1,y1,x2,y2,v_x1,v_y1,v_x2,v_y2]\n",
    "\n",
    "    Returns:\n",
    "        Data_filtered (array): A filtered data array containing all non-converging trajectories. \n",
    "    '''\n",
    "    # Creates a deep copy of the input data\n",
    "    data_filtered = deepcopy(data)\n",
    "\n",
    "    # We look for the indexes of the converging data samples\n",
    "    index = []\n",
    "\n",
    "    for i in range(9000): # i:th data sample\n",
    "        # We look for convergences every 10th time step\n",
    "        data_i = data_filtered[i][::10]\n",
    "\n",
    "        for j in range(100): #j*10:th time step\n",
    "            # The 0:th element is the time element, which we aren't interested in\n",
    "            data_ij = data_i[j][1:]\n",
    "\n",
    "            # If all positions and velocities = 0, add the index to the list\n",
    "            # and break the loop, proceeding to sample i+1\n",
    "            if all(x == 0 for x in data_ij) == True:\n",
    "                index.append(i)\n",
    "                break\n",
    "    \n",
    "    # The final 0 specifies axis 0 - the data sample axis\n",
    "    data_filtered = np.delete(data_filtered[:],index,0)\n",
    "\n",
    "    return data_filtered\n",
    "\n",
    "# Filtered data\n",
    "valid_data = data_filter(data)\n",
    "\n",
    "print('Shape of the filtered data:',valid_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(idx):\n",
    "    \"\"\"\n",
    "    Get one training instance from the data set at the index idx. \n",
    "    \n",
    "    The data is assumed to be in an array `data`.\n",
    "    \n",
    "    Args:\n",
    "        idx (int): An integer specifying which of the training example to fetch\n",
    "        \n",
    "    Returns:\n",
    "        x (array): An array of shape (time_steps, 3) which specifies the input to\n",
    "                   the neural network. The first column is the time and the second\n",
    "                   and third columns specify the (x, y) coordinates of the second\n",
    "                   particle. Note that the first particle is always assumed to be\n",
    "                   at (1, 0) and the third particle can be inferred from the first\n",
    "                   and second particle's position.\n",
    "                   \n",
    "        y (array): An array of shape (time_steps, 4) which specifies the output that \n",
    "                   is expected from the neural network.\n",
    "                   \n",
    "                   The first two columns specify the (x, y) coordinates of the first\n",
    "                   particles and the next two columns give the coordinates of the \n",
    "                   second particle for the specified time (length of the columns).\n",
    "                   The third particles position can be inferred from the first\n",
    "                   and second particle's position.\n",
    "    \"\"\"\n",
    "    # Empty arrays to be filled\n",
    "    x = np.empty((0,3))\n",
    "    y = np.empty((0,4))\n",
    "\n",
    "    # Variables are structured as [t,x1,y1,x2,y2,v_x1,v_y1,v_x2,v_y2]\n",
    "    # -> [0] = t, [3:5] = x2,y2\n",
    "    # -> [:5] = t,x1,y1,x2,y2\n",
    "    for i in range(1000): \n",
    "        x_i = [np.append(valid_data[idx][i][0], valid_data[idx][0][3:5])]\n",
    "        y_i = [valid_data[idx][i][1:5]]\n",
    "\n",
    "        x = np.concatenate((x,x_i))\n",
    "        y = np.concatenate((y,y_i))\n",
    "    \n",
    "    return x,y\n",
    "\n",
    "print('Shape of x:',get_data(0)[0].shape)\n",
    "print('Shape of y:',get_data(0)[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Division of data into training and testing/validation sets\n",
    "\n",
    "# Since the data takes long to generate, we allow for loading\n",
    "# premade data instead by setting have_data to True\n",
    "have_data = True\n",
    "\n",
    "if have_data == False:\n",
    "    # The first 90% of the data samples are used as training samples\n",
    "    # and the latter 10% as test samples\n",
    "    train_index = np.arange(0,int(.9 * len(valid_data[:,0,0])))\n",
    "    test_index  = np.arange(int(.9 * len(valid_data[:,0,0])),len(valid_data[:,0,0]))\n",
    "\n",
    "    x_train = [get_data(i)[0] for i in train_index]\n",
    "    y_train = [get_data(i)[1] for i in train_index]\n",
    "\n",
    "    x_test  = [get_data(i)[0] for i in test_index]\n",
    "    y_test  = [get_data(i)[1] for i in test_index]\n",
    "\n",
    "    x_train = np.array(x_train)\n",
    "    y_train = np.array(y_train)\n",
    "    x_test  = np.array(x_test)\n",
    "    y_test  = np.array(y_test)\n",
    "\n",
    "    # Saving the data for future use\n",
    "    np.save('x_train',x_train)\n",
    "    np.save('y_train',y_train)\n",
    "    np.save('x_test',x_test)\n",
    "    np.save('y_test',y_test)\n",
    "\n",
    "elif have_data == True:\n",
    "    x_train = np.load('x_train.npy')\n",
    "    y_train = np.load('y_train.npy')\n",
    "    x_test  = np.load('x_test.npy')\n",
    "    y_test  = np.load('y_test.npy')\n",
    "\n",
    "# Shapes of the data sets\n",
    "print('x_train: ',x_train.shape)\n",
    "print('y_train: ',y_train.shape)\n",
    "print('x_test:  ',x_test.shape)\n",
    "print('y_test:  ',y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trajectories(pred):\n",
    "    \"\"\"\n",
    "    Gets the trajectories from a predicted output pred.\n",
    "    \n",
    "    Args:\n",
    "        pred (array): An array of shape (N, 4) where N is the number of time\n",
    "                      steps. The four columns give the positions of the particles\n",
    "                      1 and 2 for all the time steps.\n",
    "    Returns:\n",
    "        p1, p2, p3 (tuple of arrays): Three arrays of dimensions (N, 2) where N is the number \n",
    "                             of time steps and the two columns for each array give \n",
    "                             the positions of the three particles (p1, p2, p3)\n",
    "    \"\"\"\n",
    "    # The following code does not work for standard Python arrays\n",
    "    pred = np.array(pred)\n",
    "\n",
    "    # p1 and p2 are simply the columns of pred\n",
    "    p1 = pred[:,[0,1]]\n",
    "    p2 = pred[:,[2,3]]\n",
    "\n",
    "    # Since p1 + p2 + p3 = [0,0]\n",
    "    p3 = -(p1+p2)\n",
    "\n",
    "    return p1,p2,p3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_trajectories(p1, p2, p3, ax=None, **kwargs):\n",
    "    \"\"\"\n",
    "    Plots trajectories for points p1, p2, p3\n",
    "    \n",
    "    Args:\n",
    "        p1, p2, p3 (array): Three arrays each of shape (n, 2) where n is the number\n",
    "                            of time steps. Each array is the (x, y) position for the\n",
    "                            particles\n",
    "        ax (axis object): Default None, in which case a new axis object is created.\n",
    "        kwargs (dict): Optional keyword arguments for plotting\n",
    "        \n",
    "    Returns:\n",
    "        ax: Axes object\n",
    "    \"\"\"\n",
    "    # The scatter plots indicate starting positions\n",
    "    ax.plot(p1[:,0],p1[:,1],color='red')\n",
    "    ax.scatter(p1[0,0],p1[0,1],color='red')\n",
    "    ax.plot(p2[:,0],p2[:,1],color='blue')\n",
    "    ax.scatter(p2[0,0],p2[0,1],color='blue')\n",
    "    ax.plot(p3[:,0],p3[:,1],color='green')\n",
    "    ax.scatter(p3[0,0],p3[0,1],color='green')\n",
    "    ax.set_xlim([-1.1,1.1])\n",
    "    ax.set_ylim([-1.1,1.1])\n",
    "\n",
    "def plot_dashed(p1, p2, p3, ax=None, **kwargs):\n",
    "    \"\"\"\n",
    "    Same as above, but dashed lines.\n",
    "    \"\"\"\n",
    "    # The scatter plots indicate starting positions\n",
    "    ax.plot(p1[:,0],p1[:,1],linestyle='dashed',color='red')\n",
    "    ax.scatter(p1[0,0],p1[0,1],color='red')\n",
    "    ax.plot(p2[:,0],p2[:,1],linestyle='dashed',color='blue')\n",
    "    ax.scatter(p2[0,0],p2[0,1],color='blue')\n",
    "    ax.plot(p3[:,0],p3[:,1],linestyle='dashed',color='green')\n",
    "    ax.scatter(p3[0,0],p3[0,1],color='green')\n",
    "    ax.set_xlim([-1.1,1.1])\n",
    "    ax.set_ylim([-1.1,1.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating our ANN\n",
    "# We use 5 hidden layers with 64 nodes each\n",
    "ANN = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.InputLayer(input_shape=(3)),\n",
    "    tf.keras.layers.Dense(64,activation='relu'),\n",
    "    tf.keras.layers.Dropout(.2),\n",
    "    tf.keras.layers.Dense(64,activation='relu'),\n",
    "    tf.keras.layers.Dropout(.2),\n",
    "    tf.keras.layers.Dense(64,activation='relu'),\n",
    "    tf.keras.layers.Dropout(.2),\n",
    "    tf.keras.layers.Dense(64,activation='relu'),\n",
    "    tf.keras.layers.Dropout(.2),\n",
    "    tf.keras.layers.Dense(64,activation='relu'),\n",
    "    tf.keras.layers.Dropout(.2),\n",
    "    tf.keras.layers.Dense(4,activation='softmax')\n",
    "])\n",
    "\n",
    "# Adding relevant settings to the model\n",
    "ANN.compile(optimizer=keras.optimizers.Adam(0.001, 0.5, 0.5),\n",
    "            loss=keras.losses.MeanAbsoluteError(),\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "# Creating tensorflow datasets and dividing them into batches\n",
    "# These will be used during training\n",
    "data_train = tf.data.Dataset.from_tensor_slices((x_train.reshape(-1,3), y_train.reshape(-1,4)))\n",
    "data_test  = tf.data.Dataset.from_tensor_slices((x_test.reshape(-1,3), y_test.reshape(-1,4)))\n",
    "\n",
    "data_train = data_train.batch(5000)\n",
    "data_test  = data_test.batch(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the pre-trained ANN made by Breen et al\n",
    "ANN_breen = tf.keras.models.load_model('Breen_NN_project2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summaries of the two models\n",
    "ANN.summary()\n",
    "ANN_breen.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training our model and saving the stats of each epoch to a .csv-file\n",
    "csv_log = CSVLogger('history_ep100_ba5000_l5_no64_CORRECTED.csv',separator=',',append=False)\n",
    "history = ANN.fit_generator(data_train, epochs=100, validation_data = data_test,\n",
    "            callbacks = [csv_log])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving and loading the trained model if needed\n",
    "\n",
    "# Set new_training to True to save the newly trained model,\n",
    "# or to False to load a model that was trained earlier\n",
    "new_training = False\n",
    "\n",
    "if new_training == True:\n",
    "    ANN.save('trained_ANN_e100_ba5000_sl5_no64_CORRECTED.h5')\n",
    "\n",
    "elif new_training == False:\n",
    "    ANN = tf.keras.models.load_model('trained_ANN_e100_ba5000_sl5_no64_CORRECTED.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison of training and testing MAEs\n",
    "\n",
    "# We use the data stored in the .csv-file from earlier\n",
    "history_load = np.genfromtxt('history_ep100_ba5000_l5_no64_CORRECTED.csv',delimiter=',',skip_header=1)\n",
    "epochs    = history_load[:,0]\n",
    "acc_train = history_load[:,1]\n",
    "MAE_train = history_load[:,2]\n",
    "acc_test  = history_load[:,3]\n",
    "MAE_test  = history_load[:,4]\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(15,10))\n",
    "ax.plot(epochs,MAE_test,label='Validation data',linestyle='dashed')\n",
    "ax.plot(epochs,MAE_train,label='Training data')\n",
    "ax.legend(fontsize=20)\n",
    "ax.set_xlabel('Epochs',fontsize=18)\n",
    "ax.set_ylabel('Mean absolute error (MAE)',fontsize=18)\n",
    "ax.tick_params(axis='both',which='major',labelsize=12)\n",
    "ax.grid()\n",
    "fig.savefig('MAE final.pdf',bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the ANNs\n",
    "breen_stats = ANN_breen.evaluate(x_test.reshape(-1,3),y_test.reshape(-1,4))\n",
    "our_stats = ANN.evaluate(x_test.reshape(-1,3),y_test.reshape(-1,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the computational cost of the two ANNs for a random sample\n",
    "rand_samp = np.random.randint(valid_data.shape[0])\n",
    "\n",
    "x = get_data(rand_samp)[0]\n",
    "\n",
    "print('Sample:',rand_samp,'\\n')\n",
    "\n",
    "print('Our computation time:')\n",
    "t_our = %timeit -o ANN.predict(x,verbose=0)\n",
    "\n",
    "print('\\nBreen\\'s computation time:')\n",
    "t_breen = %timeit -o ANN_breen.predict(x,verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANN_breen = tf.keras.models.load_model('Breen_NN_project2.h5')\n",
    "\n",
    "i = [500,7300] # One training and one validation sample\n",
    "\n",
    "xt_in,yt_data = get_data(i[0])\n",
    "xv_in,yv_data = get_data(i[1])\n",
    "\n",
    "# Training sample predictions\n",
    "t_our = ANN.predict(xt_in,verbose=0)\n",
    "t_breen = ANN_breen.predict(xt_in,verbose=0)\n",
    "\n",
    "# Validation sample predictions\n",
    "v_our = ANN.predict(xv_in,verbose=0)\n",
    "v_breen = ANN_breen.predict(xv_in,verbose=0)\n",
    "\n",
    "# Training trajectories\n",
    "pt_our = get_trajectories(t_our)\n",
    "pt_breen = get_trajectories(t_breen)\n",
    "pt_real = get_trajectories(yt_data)\n",
    "\n",
    "# Validation trajectories\n",
    "pv_our = get_trajectories(v_our)\n",
    "pv_breen = get_trajectories(v_breen)\n",
    "pv_real = get_trajectories(yv_data)\n",
    "\n",
    "fig,ax = plt.subplots(2,2,figsize=(12,12))\n",
    "\n",
    "# Training sample plots\n",
    "plot_trajectories(pt_our[0],pt_our[1],pt_our[2],ax[0,0])\n",
    "plot_dashed(pt_real[0],pt_real[1],pt_real[2],ax[0,0])\n",
    "plot_trajectories(pt_breen[0],pt_breen[1],pt_breen[2],ax[1,0])\n",
    "plot_dashed(pt_real[0],pt_real[1],pt_real[2],ax[1,0])\n",
    "\n",
    "# Validation sample plots\n",
    "plot_trajectories(pv_our[0],pv_our[1],pv_our[2],ax[0,1])\n",
    "plot_dashed(pv_real[0],pv_real[1],pv_real[2],ax[0,1])\n",
    "plot_trajectories(pv_breen[0],pv_breen[1],pv_breen[2],ax[1,1])\n",
    "plot_dashed(pv_real[0],pv_real[1],pv_real[2],ax[1,1])\n",
    "\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        ax[i,j].set_xlabel('x',fontsize='12')\n",
    "        ax[i,j].set_ylabel('y',fontsize='12')\n",
    "\n",
    "plt.rcParams['axes.grid'] = True\n",
    "\n",
    "fig.savefig('ANN validation.pdf',bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploring the chaotic nature of the predictions\n",
    "\n",
    "# We want to predict 100 trajectories that are slightly disturbed\n",
    "# from the starting position. Let's use a circle like Breen et al.\n",
    "def circ(r,N=100):\n",
    "    '''\n",
    "    Generates N evenly spaced points on the circumference\n",
    "    of a circle with radius r centered at (0,0).\n",
    "    The first column is 0 solely so it plays better better with x-arrays.\n",
    "    '''\n",
    "    return np.array([[0,r*np.cos(t),r*np.sin(t)] for t in np.linspace(0,2*np.pi,N)])\n",
    "\n",
    "# Fetching the sample we want to try\n",
    "x_true,y_true = get_data(2543)\n",
    "r = 0.01 # radius\n",
    "N = 100 # amount of points on the circle\n",
    "\n",
    "# Creating the distrubed states\n",
    "x_chaos = np.array([x_true+circ(r)[i] for i in range(N)])\n",
    "print(x_chaos.shape)\n",
    "\n",
    "# Predict the 100 disturbed states\n",
    "chaos_breen = np.array([ANN_breen.predict(x_chaos[i],verbose=0) for i in range(N)])\n",
    "chaos_our   = np.array([ANN.predict(x_chaos[i],verbose=0) for i in range(N)])\n",
    "\n",
    "# Get p2 for all predictions\n",
    "traj_breen = np.array([get_trajectories(chaos_breen[i]) for i in range(N)])\n",
    "traj_our = np.array([get_trajectories(chaos_our[i]) for i in range(N)])\n",
    "\n",
    "# Breen plot\n",
    "fig,ax = plt.subplots(figsize=(10,8))\n",
    "for disturbed_state in range(100):\n",
    "    for time in [0,200,400,600,800,999]:\n",
    "        ax.scatter(traj_breen[disturbed_state][1][time,0],traj_breen[disturbed_state][1][time,1],color='blue')\n",
    "\n",
    "# Our plot\n",
    "for disturbed_state in range(100):\n",
    "    for time in [0,200,400,600,800,999]:\n",
    "        ax.scatter(traj_our[disturbed_state][1][time,0],traj_our[disturbed_state][1][time,1],color='red')\n",
    "\n",
    "# True trajectory\n",
    "true_traj = get_trajectories(y_true)\n",
    "ax.plot(true_traj[1][:,0],true_traj[1][:,1])\n",
    "ax.set_xlim([-.5,.6])\n",
    "ax.set_ylim([-.15,.75])\n",
    "ax.set_xlabel('x',fontsize=15)\n",
    "ax.set_ylabel('y',fontsize=15)\n",
    "\n",
    "#fig.savefig('chaos.pdf',bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Part 2 (extra task)\n",
    "* Create a helper function that extracts the velocity at each time step.\n",
    "* Compute the potential energy and the kinetic energy at each time step. The units are chosen such that the total energy is just the sum of these two.\n",
    "* Check for the conservation of energy in: (i) the data generated by `Brutus` (note that the velocities are given in the last few columns of the data file); (ii) the pre-trained model from the paper; (iii) your ANN model.\n",
    "* Create a custom loss function and re-train your model to be trained (constrained) by energy conservation. (Unfortunately, you should not expect any significant improvement in the model with the energy-conserving loss function.)\n",
    "* Reproduce Fig. 6 (with the green line being replaced by your energy-conserving ANN). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments and hints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* You should probably define helper functions that compute potential energies, velocities and kinetic energies for points along a trajectory. Note that the masses start from rest.\n",
    "* In order to construct a `tensorflow` custom loss you will need `@tf.function()` versions of the same helper functions. Within these you can only use `tensforflow` methods such as: `tf.concat()` and `tf.math.sqrt()` (i.e., `numpy` methods cannot be used for these functions). \n",
    "* See (suggested) `tensorflow` helper functions docstrings below. \n",
    "* You can define a custom loss function that assumes a batch size that is equal to the number of time steps in a trajectory. You should also make sure that the data is not shuffled so that a batch always corresponds to a trajectory. Then you can use a global time array to be used to extract velocities and therefore kinetic energies.\n",
    "* See a suggested structure of the custom loss below. You might get problems with very large kinetic and potential energies for trajectories with close encounters. A suggestion is to clip very large values of `energy_error` in the return statement of the custom energy loss by `tf.clip_by_value(energy_error, -1e6, 1e6)`. \n",
    "* The custom loss is then used in `model.compile()` via the argument `loss=custom_energy_loss`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper function docstrings\n",
    "The following are just suggestions. You are allowed to construct a solution using your own helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "@tf.function()\n",
    "def tf_ediff1d(a):\n",
    "    \"\"\"\n",
    "    TensorFlow equivalent of 1D difference.\n",
    "    \n",
    "    Args:\n",
    "        a (array): A TensorFlow array of dimension (N, 1)\n",
    "        \n",
    "    Returns:\n",
    "        diff (array): An array which gives the difference of successive elements of a.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError\n",
    "\n",
    "@tf.function()\n",
    "def tf_compute_velocities(t, p):\n",
    "    \"\"\"\n",
    "    Computes the velocities of the particles from the trajectories starting from rest.\n",
    "    \n",
    "    Args:\n",
    "        t (array[float]): An array of shape (N, 1) giving time steps.\n",
    "        p (array): An array of dimension (N, 2) where N is the number \n",
    "                   of time steps and the two columns give the coordinates\n",
    "                   of the particle.\n",
    "                             \n",
    "    Returns:\n",
    "        v (array): An array of dimension (N, 2) where N is the number \n",
    "                   of time steps and the two columns for each array give \n",
    "                   the x and y components for the instantaneous velocity\n",
    "                   of the particle.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError\n",
    "\n",
    "@tf.function()\n",
    "def tf_compute_kinetic_energy(v):\n",
    "    \"\"\"\n",
    "    Computes the kinetic energy for the given velocity vectors\n",
    "    \n",
    "    Args:\n",
    "        v (array): A (N,2) array of veolcities for N time steps.\n",
    "        \n",
    "    Returns:\n",
    "        ke (array): An array of shape (N, 1) giving the kinetic energies at each time step.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError\n",
    "\n",
    "@tf.function()\n",
    "def tf_compute_potential_energy(p1, p2, p3):\n",
    "    \"\"\"\n",
    "    Computes the potential energy for the given position vectors. The value of the \n",
    "    gravitational constant is taken as 1 (G=1). The masses are the same value (m=1)\n",
    "    \n",
    "    Args:\n",
    "        p1, p2, p3 (tuple of arrays): Three arrays of dimensions (N, 2) where N is the number \n",
    "                             of time steps and the two columns for each array give \n",
    "                             the positions of the three particles (p1, p2, p3)\n",
    "                             \n",
    "    Returns\n",
    "        pe (array): An array of shape(N, 1) giving the potential energy at each time step\n",
    "        \n",
    "    \"\"\"\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suggested structure of the custom loss function\n",
    "tlist = ... # Which will be used in the custom loss\n",
    "\n",
    "def custom_loss(y, y_pred):\n",
    "    \"\"\"\n",
    "    A custom loss function computing error in energy conservation.\n",
    "    \"\"\"\n",
    "    predicted_positions = ... # Obtained from y_pred\n",
    "    predicted_velocities = ... # Obtained from predicted_positions, tlist\n",
    "\n",
    "    initial_potential_energy = ... # Obtained from positions (y)\n",
    "\n",
    "    ke_predicted_trajectory = ...# Obtained from predicted_velocities\n",
    "    pe_predicted_trajectory = ... # Obtained from predicted_positions\n",
    "\n",
    "    error = (ke_predicted_trajectory + pe_predicted_trajectory -\n",
    "                 initial_potential_energy)\n",
    "\n",
    "    energy_loss = tf.reduce_mean(tf.abs(error))\n",
    "    # The relative weight ofthe two terms in the custom loss might be tuned.\n",
    "    return tf.keras.losses.MeanAbsoluteError()(y, y_pred) + 0.001*energy_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "e4141dd47493d41e3aa9178167523f3ccba7cb41484e039197de5dc2926091f3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
